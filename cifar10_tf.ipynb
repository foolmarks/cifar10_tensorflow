{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Blue;\">CIFAR-10 with TensorFlow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "This example shows how to build a simple CNN for classifying the CIFAR10 dataset using the TensorFlow layers API, then how to train and evaluate it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First, we import the necessary Python packages and print out the versions of the TensorFlow & Keras packages.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "\n",
    "print(\"Tensorflow version. \", tf.VERSION)\n",
    "print(\"Keras version. \", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create some directories for the TensorBoard event logs and the checkpoints...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "GRAPH_FILE_BIN = 'graph.pb'\n",
    "GRAPH_FILE_TXT = 'graph.pbtxt'\n",
    "CHKPT_FILE = 'float_model.ckpt'\n",
    "FROZEN_GRAPH_FILE = 'frozen_graph.pb'\n",
    "\n",
    "CHKPT_DIR = os.path.join(SCRIPT_DIR, 'chkpts')\n",
    "TB_LOG_DIR = os.path.join(SCRIPT_DIR, 'tb_logs')\n",
    "FREEZE_DIR = os.path.join(SCRIPT_DIR, 'freeze')\n",
    "CHKPT_PATH = os.path.join(CHKPT_DIR, CHKPT_FILE)\n",
    "\n",
    "# create a directory for the TensorBoard data if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(TB_LOG_DIR)):\n",
    "    shutil.rmtree(TB_LOG_DIR)\n",
    "os.makedirs(TB_LOG_DIR)\n",
    "print(\"Directory \" , TB_LOG_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "# create a directory for the checkpoints if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(CHKPT_DIR)):\n",
    "    shutil.rmtree(CHKPT_DIR)\n",
    "os.makedirs(CHKPT_DIR)\n",
    "print(\"Directory \" , CHKPT_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "# create a directory for the frozen graph if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(FREEZE_DIR)):\n",
    "    shutil.rmtree(FREEZE_DIR)\n",
    "os.makedirs(FREEZE_DIR)\n",
    "print(\"Directory \" , FREEZE_DIR ,  \"created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set up the learning rate for the Optimizer, the number of epochs and the batch size. Note that the number of epochs is et to a very low value so that the Notebook can be run quickly, this should really be set to a much higher number, perphas as ahigh as 10000.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNRATE = 0.0001\n",
    "EPOCHS = 3\n",
    "BATCHSIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">Data Wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Download the CIFAR-10 dataset using the Keras function. What you get is a dataset that has been split into 50k images & labels for training, 10k images and labels for test. The 'images' are actually numpy arrays with the datatype set to 8bit unsigned integer. We scale this image data back to the range 0:1.0 by dividing by 255.0. The labels are also integers, so we one-hot encode them using the Keras 'to_categorical()' method.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 datset has 60k images. Training set is 50k, test set is 10k.\n",
    "# Each image is 32x32 pixels RGB\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale image data from range 0:255 to range 0:1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# calculate total number of batches in the training data\n",
    "total_batches = int(len(x_train)/BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Computational Graph</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "The placeholders for inputting data have shapes that match the modified datasets. The 'x' placeholder takes in the 32pixel x 32pixel RGB images (..actually numpy arrays..) and so has shape [None, 32, 32, 3].  The 'y' placeholder takes in the one-hot encoded labels.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3], name='images_in')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='labels_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Now we define our simple CNN as a series of layers..3 sets of 2D convolution and max pooling layers, then a flatten layer before a final fully connected layer with softmax activation and 10 outputs.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(x):\n",
    "  conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=3, kernel_initializer=tf.glorot_uniform_initializer(), activation=tf.nn.relu, name='conv1')\n",
    "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=2, strides=2, name='pool1')\n",
    "  conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=3, kernel_initializer=tf.glorot_uniform_initializer(), activation=tf.nn.relu, name='conv2')\n",
    "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=2, strides=2, name='pool2')\n",
    "  conv3 = tf.layers.conv2d(inputs=pool2, filters=128, kernel_size=3, kernel_initializer=tf.glorot_uniform_initializer(), activation=tf.nn.relu, name='conv3')\n",
    "  pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=2, strides=2, name='pool3')\n",
    "  flat1 = tf.layers.flatten(inputs=pool3,name='flat1')\n",
    "  fc1 = tf.layers.dense(inputs=flat1, units=1024, kernel_initializer=tf.glorot_uniform_initializer(), activation=tf.nn.relu, name='fc1')\n",
    "  prediction = tf.layers.dense(inputs=fc1, units=10, kernel_initializer=tf.glorot_uniform_initializer(), activation=tf.nn.softmax, name='prediction')\n",
    "\n",
    "  return prediction\n",
    "\n",
    "# build the network, input comes from the 'x' placeholder\n",
    "prediction = cnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The loss function is a cross entropy function for classification which accepts labels in one-hot format (..which explains why we one-hot encoded the labels earlier..). The training optimizer is an Adaptive Momentum type.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a cross entropy loss function\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=prediction, onehot_labels=y))\n",
    "\n",
    "# Define the optimizer function\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNRATE).minimize(loss)\n",
    "\n",
    "# Check to see if predictions match the labels\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    " # Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will collect the loss and accuracy data for displaying in TensorBoard along with the images that are fed into the 'x' placeholder.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard data collection\n",
    "tf.summary.scalar('cross_entropy_loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.image('input_images', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We define an instance of a saver object which will be used inside our session to save the trained model checkpoint.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up saver object\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Session</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inside the session, we initialize all the variables then loop through the number of epochs, sending the training data into the 'x' and 'y' placeholders.\n",
    "\n",
    "When we exit the training loop, the final accuracy is calculated and then the final trained model is saved as a checkpoint and as a graph in a protobuf text file.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = tf.summary.FileWriter(TB_LOG_DIR, sess.graph)\n",
    "    tb_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Training cycle with training data\n",
    "    for epoch in range(EPOCHS):\n",
    "        print (\"Epoch:\", epoch)\n",
    "\n",
    "        # process all batches\n",
    "        for i in range(total_batches):\n",
    "            \n",
    "            # fetch a batch from training dataset\n",
    "            batch_x, batch_y = x_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE], y_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE]\n",
    "\n",
    "            # Run graph for optimization, loss, accuracy - i.e. do the training\n",
    "            _, acc, s = sess.run([optimizer, accuracy, tb_summary], feed_dict={x: batch_x, y: batch_y})\n",
    "            writer.add_summary(s, (epoch*total_batches + i))\n",
    "            # Display accuracy per 100 batches\n",
    "            if i % 100 == 0:\n",
    "              print (\" Batch:\", i, 'Training accuracy: ', acc)\n",
    "\n",
    "    print(\"Training Finished!\")\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # Evaluation cycle with test data\n",
    "    print (\"Final Accuracy with test set:\", sess.run(accuracy, feed_dict={x: x_test[:1000], y: y_test[:1000]}))\n",
    "\n",
    "    # save checkpoint & graph file as binary & text protobuf\n",
    "    save_path = saver.save(sess, os.path.join(CHKPT_DIR, CHKPT_FILE) )\n",
    "    tf.train.write_graph(sess.graph_def, CHKPT_DIR, GRAPH_FILE_BIN, as_text=False)\n",
    "    tf.train.write_graph(sess.graph_def, CHKPT_DIR, GRAPH_FILE_TXT, as_text=True)\n",
    "\n",
    "    # freeze the saved graph - converts variables to constants & removes training nodes\n",
    "    freeze_graph.freeze_graph(input_graph=os.path.join(CHKPT_DIR,GRAPH_FILE_BIN),\n",
    "                              input_saver='',\n",
    "                              input_binary = True,\n",
    "                              input_checkpoint = os.path.join(CHKPT_DIR, CHKPT_FILE),\n",
    "                              output_node_names = 'prediction/Softmax',\n",
    "                              restore_op_name ='save/restore_all',\n",
    "                              filename_tensor_name = 'save/Const:0',\n",
    "                              output_graph = os.path.join(FREEZE_DIR,FROZEN_GRAPH_FILE),\n",
    "                              clear_devices = True,\n",
    "                              initializer_nodes = '')\n",
    "\n",
    "\n",
    "#  Session ended\n",
    "\n",
    "print('FINISHED!')\n",
    "print('Run `tensorboard --logdir=%s --port 6006 --host localhost` to see the results.' % TB_LOG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
